{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Chapter 17. Limited Dependent Variable Models and Sample Selection Corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Chapter 7, we studied the linear probability model, which is simply an application of the multiple regression model to a binary dependent variable. A binary dependent variable is an example of a limited dependent variable (LDV) . An LDV is broadly defined as a dependent variable whose range of values is substantively restricted. A binary variable takes on only two values, zero and one. In Section 7-7, we discussed the interpretation of multiple regression estimates for generally discrete response variables, focusing on the case where y takes on a small number of integer values&#8212;for example, the number of times a young man is arrested during a year or the number of children born to a woman. Elsewhere, we have encountered several other limited dependent variables, including the percentage of people participating in a pension plan (which must be between zero and 100) and college grade point average (which is between zero and 4.0 at most colleges)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most economic variables we would like to explain are limited in some way, often because they must be positive. For example, hourly wage, housing price, and nominal interest rates must be greater than zero. But not all such variables need special treatment. If a strictly positive variable takes on many different values, a special econometric model is rarely necessary. When y is discrete and takes on a small number of values, it makes no sense to treat it as an approximately continuous variable. Discreteness of y does not in itself mean that linear models are inappropriate. However, as we saw in Chapter 7 for binary response, the linear probability model has certain drawbacks. In Section 17-1, we discuss logit and probit models, which overcome the shortcomings of the LPM; the disadvantage is that they are more difficult to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other kinds of limited dependent variables arise in econometric analysis, especially when the behavior of individuals, families, or firms is being modeled. Optimizing behavior often leads to a corner solution response for some nontrivial fraction of the population. That is, it is optimal to choose a zero quantity or dollar value, for example. During any given year, a significant number of families will make zero charitable contributions. Therefore, annual family charitable contributions has a population distribution that is spread out over a large range of positive values, but with a pileup at the value zero. Although a linear model could be appropriate for capturing the expected value of charitable contributions, a linear model will likely lead to negative predictions for some families. Taking the natural log is not possible because many observations are zero. The Tobit model, which we cover in Section 17-2, is explicitly designed to model corner solution dependent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important kind of LDV is a count variable, which takes on nonnegative integer values. Section 17-3 illustrates how Poisson regression models are well suited for modeling count variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, we encounter limited dependent variables due to data censoring, a topic we introduce in Section 17-4. The general problem of sample selection, where we observe a nonrandom sample from the underlying population, is treated in Section 17-5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limited dependent variable models can be used for time series and panel data, but they are most often applied to cross-sectional data. Sample selection problems are usually confined to cross- sectional or panel data. We focus on cross-sectional applications in this chapter. Wooldridge (2010) analyzes these problems in the context of panel data models and provides many more details for cross-sectional and panel data applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17-1. Logit and Probit Models for Binary Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear probability model is simple to estimate and use, but it has some drawbacks that we dis- cussed in Section 7-5. The two most important disadvantages are that the fitted probabilities can be less than zero or greater than one and the partial effect of any explanatory variable (appearing in level form) is constant. These limitations of the LPM can be overcome by using more sophisticated binary response models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a binary response model, interest lies primarily in the response probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(y=1|x)=P(y=1x_1,x_2,\\ldots,x_k) \\tag{17.1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we use x to denote the full set of explanatory variables. For example, when y is an employment indicator, x might contain various individual characteristics such as education, age, marital status, and other factors that affect employment status, including a binary indicator variable for participation in a recent job training program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.1a Specifying Logit and Probit Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the LPM, we assume that the response probability is linear in a set of parameters, \\Beta_j; see equation (7.27). To avoid the LPM limitations, consider a class of binary response models of the form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(y=1|x)=G(\\beta_0+\\beta_1*x_1+\\ldots+\\beta_k*x_k)=G(\\beta_0+x \\beta) \\tag{17.2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where G is a function taking on values strictly between zero and one: $0 < G(z) < 1$ for all real numbers z . This ensures that the estimated response probabilities are strictly between zero and one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various nonlinear functions have been suggested for the function G to make sure that the probabilities are between zero and one. The two we will cover here are used in the vast majority of applications (along with the LPM). In the logit model , G is the logistic function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "G(z)=exp(z)/[1+exp(z)]= \\Lambda(z) \\tag{17.3}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is between zero and one for all real numbers z . This is the cumulative distribution function (cdf) for a standard logistic random variable. In the probit model , G is the standard normal cdf, which is expressed as an integral:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "G(z)=\\Phi(z) \\equiv \\int_{-\\infty}^{z} \\phi(v) dv \\tag{17.4}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\phi(z)$ is the normal density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most applications of binary response models, the primary goal is to explain the effects of the $x_j$ on the response probability $P(y=1|x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If, say, $x_1$ is a binary explanatory variable, then the partial effect from changing $x_1$ from zero to one, holding all other variables fixed, is simply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "G(\\beta_0+\\beta_1+\\beta_2 x_2+\\ldots+\\beta_k x_k)-(\\beta_0+\\beta_2 x_2+\\ldots+\\beta_k x_k) \\tag{17.8}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that knowing the sign of $\\beta_1$ is sufficient for determining whether the program had a positive or negative effect. But to find the magnitude of the effect, we have to estimate the quantity in (17.8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.1b Maximum Likelihood Estimation of Logit and Probit Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should we estimate nonlinear binary response models? To estimate the LPM, we can use ordinary least squares (see Wooldridge Section 7-5) or, in some cases, weighted least squares (see Section 8-5). Because of the nonlinear nature of $E(y|x)$ , OLS and WLS are not applicable. We could use nonlinear versions of these methods, but it is no more difficult to use maximum likelihood estimation (MLE) (see Appendix 17A in Wooldridge for a brief discussion)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.1d Interpreting the Logit and Probit Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given modern computers, from a practical perspective the most difficult aspect of logit or probit models is presenting and interpreting the results. The coefficient estimates, their standard errors, and the value of the log-likelihood function are reported by all software packages that do logit and probit, and these should be reported in any application. The coefficients give the signs of the partial effects of each $x_j$ on the response probability, and the statistical significance of $x_j$ is determined by whether we can reject $H_0: \\beta_j=0$ at a sufficiently small significance level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, we want to estimate the effects of the $x_j$ on the response probabilities, $P(y=1|x)$. If $x_j$ is (roughly) continuous, then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\Delta \\hat P(y=1|x) \\approx [g(\\hat \\beta_0+x\\hat \\beta)\\hat \\beta_j]\\Delta x_j \\tag{17.13}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for \"small\" changes in $x_j$ . So, for $\\Delta x_j=1$, the change in the estimated success probability is roughly $g(\\hat \\beta_0+x \\hat \\beta)\\hat \\beta_j$. Compared with the linear probability model, the cost of using probit and logit models is that the partial effects in equation (17.13) are harder to summarize because the scale factor, $g(\\hat \\beta_0+x \\hat \\beta)$ , depends on x (that is, on all of the explanatory variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick summary for getting at the magnitudes of the partial effects, it is handy to have a single scale factor that can be used to multiply each $\\beta_j$ (or at least those coefficients on roughly continuous variables). One method, commonly used in econometrics packages that routinely estimate probit and logit models, is to replace each explanatory variable with its sample average. In other words, the adjustment factor is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "g(\\hat \\beta_0+\\bar x \\hat \\beta)=g(\\hat \\beta_0+\\hat \\beta_1 \\bar x_1+\\hat \\beta_2 \\bar x_2+\\ldots+\\beta_k \\bar x_k) \\tag{17.14}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $g(\\bullet)$ is the standard normal density in the probit case and $g(z=)exp(z)/[1+exp(z)]^2$ in the logit case. The idea behind (17.14) is that, when it is multiplied by $\\hat \\beta_j$ , we obtain the partial effect of $x_j$ for the \"average\" person in the sample. Thus, if we multiply a coefficient by (17.14), we generally obtain the partial effect at the average (PEA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A different approach to computing a scale factor circumvents the issue of which values to plug in for the explanatory variables. Instead, the second scale factor results from averaging the individual partial effects across the sample, leading to what is called the average partial effect (APE) or, some- times, the average marginal effect (AME)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wooldridge Example 17.1 Married Women's Labor Force Participation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the data on 753 married women in MROZ to estimate the labor force participation model from Example 8.8 -see also Section 7-5- by logit and probit. We also report the linear probability model estimates from Example 8.8, using the heteroskedasticity-robust standard errors. The results, with standard errors in parentheses, are given in Table 17.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing package into '/home/nbuser/R'\n",
      "(as 'lib' is unspecified)\n",
      "Installing package into '/home/nbuser/R'\n",
      "(as 'lib' is unspecified)\n",
      "also installing the dependency 'betareg'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================================================\n",
      "                                Dependent variable:            \n",
      "                    -------------------------------------------\n",
      "                                       inlf                    \n",
      "                              OLS           logistic   probit  \n",
      "                              (1)              (2)       (3)   \n",
      "---------------------------------------------------------------\n",
      "nwifeinc                   -0.003**         -0.021**  -0.012** \n",
      "                            (0.001)          (0.008)   (0.005) \n",
      "                                                               \n",
      "educ                       0.038***         0.221***  0.131*** \n",
      "                            (0.007)          (0.043)   (0.025) \n",
      "                                                               \n",
      "exper                      0.039***         0.206***  0.123*** \n",
      "                            (0.006)          (0.032)   (0.019) \n",
      "                                                               \n",
      "I(exper2)                  -0.001***        -0.003*** -0.002***\n",
      "                           (0.0002)          (0.001)   (0.001) \n",
      "                                                               \n",
      "age                        -0.016***        -0.088*** -0.053***\n",
      "                            (0.002)          (0.015)   (0.008) \n",
      "                                                               \n",
      "kidslt6                    -0.262***        -1.443*** -0.868***\n",
      "                            (0.034)          (0.204)   (0.118) \n",
      "                                                               \n",
      "kidsge6                      0.013            0.060     0.036  \n",
      "                            (0.013)          (0.075)   (0.044) \n",
      "                                                               \n",
      "Constant                   0.586***           0.425     0.270  \n",
      "                            (0.154)          (0.860)   (0.508) \n",
      "                                                               \n",
      "---------------------------------------------------------------\n",
      "Observations                  753              753       753   \n",
      "R2                           0.264                             \n",
      "Adjusted R2                  0.257                             \n",
      "Log Likelihood                              -401.765  -401.302 \n",
      "Akaike Inf. Crit.                            819.530   818.604 \n",
      "Residual Std. Error    0.427 (df = 745)                        \n",
      "F Statistic         38.218*** (df = 7; 745)                    \n",
      "===============================================================\n",
      "Note:                               *p<0.1; **p<0.05; ***p<0.01\n"
     ]
    }
   ],
   "source": [
    "install.packages('stargazer'); install.packages('mfx')\n",
    "library(foreign);library(car); library(lmtest); library(stargazer)  # for robust SE\n",
    "mroz <- read.dta(\"https://github.com/thousandoaks/Wooldridge/blob/master/mroz.dta?raw=true\")\n",
    "\n",
    "# Estimate linear probability model\n",
    "linprob <- lm(inlf~nwifeinc+educ+exper+I(exper^2)+age+kidslt6+kidsge6,data=mroz)\n",
    "\n",
    "# Estimate logit model\n",
    "logitres<-glm(inlf~nwifeinc+educ+exper+I(exper^2)+age+kidslt6+kidsge6,\n",
    "                                family=binomial(link=logit),data=mroz)\n",
    "\n",
    "# Estimate probit model\n",
    "probitres<-glm(inlf~nwifeinc+educ+exper+I(exper^2)+age+kidslt6+kidsge6,\n",
    "                                family=binomial(link=probit),data=mroz)\n",
    "\n",
    "\n",
    "stargazer(linprob,logitres, probitres,type=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimates from the three models tell a consistent story. The signs of the coefficients are the same across models, and the same variables are statistically significant in each model. The pseudo R -squared for the LPM is just the usual R -squared reported for OLS; for logit and probit, the pseudo R -squared is the measure based on the log-likelihoods described earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table reports the average partial effects for all explanatory variables and for each of the three estimated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #Automatic APE calculations with package mfx\n",
    "library(mfx)\n",
    "logitpartialeffects<-logitmfx(inlf~nwifeinc+educ+exper+I(exper^2)+age+kidslt6+kidsge6, \n",
    "                                              data=mroz, atmean=FALSE)\n",
    "probitpartialeffects<-probitmfx(inlf~nwifeinc+educ+exper+I(exper^2)+age+kidslt6+kidsge6, \n",
    "                                              data=mroz, atmean=FALSE)\n",
    "linpartialeffects <- lm(inlf~nwifeinc+educ+exper+I(exper^2)+age+kidslt6+kidsge6,data=mroz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>linpartialeffects.coefficients..1.</th><th scope=col>logitpartialeffects.mfxest...1.</th><th scope=col>probitpartialeffects.mfxest...1.</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>nwifeinc</th><td>-0.0034051689</td><td>-0.0038118134</td><td>-0.003616175 </td></tr>\n",
       "\t<tr><th scope=row>educ</th><td> 0.0379953029</td><td> 0.0394965237</td><td> 0.039370095 </td></tr>\n",
       "\t<tr><th scope=row>exper</th><td> 0.0394923894</td><td> 0.0367641055</td><td> 0.037097345 </td></tr>\n",
       "\t<tr><th scope=row>I(exper^2)</th><td>-0.0005963119</td><td>-0.0005632587</td><td>-0.000567546 </td></tr>\n",
       "\t<tr><th scope=row>age</th><td>-0.0160908062</td><td>-0.0157193607</td><td>-0.015895665 </td></tr>\n",
       "\t<tr><th scope=row>kidslt6</th><td>-0.2618104670</td><td>-0.2577536552</td><td>-0.261153464 </td></tr>\n",
       "\t<tr><th scope=row>kidsge6</th><td> 0.0130122345</td><td> 0.0107348185</td><td> 0.010828887 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       "  & linpartialeffects.coefficients..1. & logitpartialeffects.mfxest...1. & probitpartialeffects.mfxest...1.\\\\\n",
       "\\hline\n",
       "\tnwifeinc & -0.0034051689 & -0.0038118134 & -0.003616175 \\\\\n",
       "\teduc &  0.0379953029 &  0.0394965237 &  0.039370095 \\\\\n",
       "\texper &  0.0394923894 &  0.0367641055 &  0.037097345 \\\\\n",
       "\tI(exper\\textasciicircum{}2) & -0.0005963119 & -0.0005632587 & -0.000567546 \\\\\n",
       "\tage & -0.0160908062 & -0.0157193607 & -0.015895665 \\\\\n",
       "\tkidslt6 & -0.2618104670 & -0.2577536552 & -0.261153464 \\\\\n",
       "\tkidsge6 &  0.0130122345 &  0.0107348185 &  0.010828887 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | linpartialeffects.coefficients..1. | logitpartialeffects.mfxest...1. | probitpartialeffects.mfxest...1. | \n",
       "|---|---|---|---|---|---|---|\n",
       "| nwifeinc | -0.0034051689 | -0.0038118134 | -0.003616175  | \n",
       "| educ |  0.0379953029 |  0.0394965237 |  0.039370095  | \n",
       "| exper |  0.0394923894 |  0.0367641055 |  0.037097345  | \n",
       "| I(exper^2) | -0.0005963119 | -0.0005632587 | -0.000567546  | \n",
       "| age | -0.0160908062 | -0.0157193607 | -0.015895665  | \n",
       "| kidslt6 | -0.2618104670 | -0.2577536552 | -0.261153464  | \n",
       "| kidsge6 |  0.0130122345 |  0.0107348185 |  0.010828887  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "           linpartialeffects.coefficients..1. logitpartialeffects.mfxest...1.\n",
       "nwifeinc   -0.0034051689                      -0.0038118134                  \n",
       "educ        0.0379953029                       0.0394965237                  \n",
       "exper       0.0394923894                       0.0367641055                  \n",
       "I(exper^2) -0.0005963119                      -0.0005632587                  \n",
       "age        -0.0160908062                      -0.0157193607                  \n",
       "kidslt6    -0.2618104670                      -0.2577536552                  \n",
       "kidsge6     0.0130122345                       0.0107348185                  \n",
       "           probitpartialeffects.mfxest...1.\n",
       "nwifeinc   -0.003616175                    \n",
       "educ        0.039370095                    \n",
       "exper       0.037097345                    \n",
       "I(exper^2) -0.000567546                    \n",
       "age        -0.015895665                    \n",
       "kidslt6    -0.261153464                    \n",
       "kidsge6     0.010828887                    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's print it in a single table for comparison\n",
    "data.frame(linpartialeffects$coefficients[-1],logitpartialeffects$mfxest[,1],probitpartialeffects$mfxest[,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is clear from the table, the APEs are very similar for all explanatory variables across all three models. The biggest difference between the LPM model and the logit and probit models is that the LPM assumes constant marginal effects for educ , kidslt6 , and so on, while the logit and probit models imply diminishing magnitudes of the partial effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17-2. The Tobit Model for Corner Solution Responses "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the chapter introduction, another important kind of limited dependent variable is a corner solution response. Such a variable is zero for a nontrivial fraction of the population but is roughly continuously distributed over positive values. An example is the amount an individual spends on alcohol in a given month. In the population of people over age 21 in the United States, this variable takes on a wide range of values. For some significant fraction, the amount spent on alcohol is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let y be a variable that is essentially continuous over strictly positive values but that takes on a value of zero with positive probability. Nothing prevents us from using a linear model for y . In fact, a linear model might be a good approximation to \n",
    "$E(y|x_1,x_2,\\ldots,x_k)$, especially for $x_j$ near the mean values. But we would possibly obtain negative fitted values, which leads to negative predictions for y ; this is analogous to the problems with the LPM for binary outcomes. Also, the assumption that an explanatory variable appearing in level form has a constant partial effect on $E(y|x)$ can be misleading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tobit model is quite convenient for these purposes. Typically, the Tobit model expresses the observed response, y , in terms of an underlying latent variable:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "y^* =\\beta_0+x \\beta+u,u|x \\sim Normal(0,\\sigma^2) \\tag{17.18}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "y =max(0,y^*) \\tag{17.19}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latent variable y p satisfies the classical linear model assumptions; in particular, it has a normal, homoskedastic distribution with a linear conditional mean. Equation (17.19) implies that the observed variable, y , equals $y^*$ when $y^* \\geq 0$, but $y=0$ when $y^* \\leq 0$. Because $y^*$ is normally distributed, y has a continuous distribution over strictly positive values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.2a Interpreting the Tobit Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using modern computers, it is usually not much more difficult to obtain the maximum likelihood estimates for Tobit models than the OLS estimates of a linear model. Further, the outputs from Tobit and OLS are often similar. This makes it tempting to interpret the $\\hat \\beta_j$ from Tobit as if these were estimates from a linear regression. Unfortunately, things are not so easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From equation (17.18), we see that the $\\beta_j$ measure the partial effects of the $x_j$ on $E(y^*|x)$ , where $y^*$ is the latent variable. Sometimes, $y^*$ has an interesting economic meaning, but more often it does not. The variable we want to explain is y , as this is the observed outcome (such as hours worked or amount of charitable contributions). For example, as a policy matter, we are interested in the sensitivity of hours worked to changes in marginal tax rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wooldridge Example 17.2 Married Women's Annual Labor Supply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file MROZ includes data on hours worked for 753 married women, 428 of whom worked for a wage outside the home during the year; 325 of the women worked zero hours. For the women who worked positive hours, the range is fairly broad, extending from 12 to 4,950. Thus, annual hours worked is a good candidate for a Tobit model. We also estimate a linear model (using all 753 observations) by OLS. The following code performs the calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing package into '/home/nbuser/R'\n",
      "(as 'lib' is unspecified)\n",
      "also installing the dependencies 'bdsmatrix', 'glmmML', 'plm'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "install.packages('censReg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(foreign)\n",
    "mroz <- read.dta(\"https://github.com/thousandoaks/Wooldridge/blob/master/mroz.dta?raw=true\")\n",
    "\n",
    "# Estimate Tobit model using censReg:\n",
    "library(censReg)\n",
    "TobitRes <- censReg(hours~nwifeinc+educ+exper+I(exper^2)+ \n",
    "                                    age+kidslt6+kidsge6, data=mroz )\n",
    "#summary(TobitRes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linpartialeffects <- lm(hours~nwifeinc+educ+exper+I(exper^2)+age+kidslt6+kidsge6,data=mroz)\n",
    "#linpartialeffects$coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>linpartialeffects.coefficients</th><th scope=col>head.TobitRes.estimate...1.</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>(Intercept)</th><td>1330.4824036</td><td> 965.305296 </td></tr>\n",
       "\t<tr><th scope=row>nwifeinc</th><td>  -3.4466356</td><td>  -8.814243 </td></tr>\n",
       "\t<tr><th scope=row>educ</th><td>  28.7611246</td><td>  80.645605 </td></tr>\n",
       "\t<tr><th scope=row>exper</th><td>  65.6725131</td><td> 131.564299 </td></tr>\n",
       "\t<tr><th scope=row>I(exper^2)</th><td>  -0.7004939</td><td>  -1.864158 </td></tr>\n",
       "\t<tr><th scope=row>age</th><td> -30.5116345</td><td> -54.405012 </td></tr>\n",
       "\t<tr><th scope=row>kidslt6</th><td>-442.0899082</td><td>-894.021740 </td></tr>\n",
       "\t<tr><th scope=row>kidsge6</th><td> -32.7792266</td><td> -16.217996 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & linpartialeffects.coefficients & head.TobitRes.estimate...1.\\\\\n",
       "\\hline\n",
       "\t(Intercept) & 1330.4824036 &  965.305296 \\\\\n",
       "\tnwifeinc &   -3.4466356 &   -8.814243 \\\\\n",
       "\teduc &   28.7611246 &   80.645605 \\\\\n",
       "\texper &   65.6725131 &  131.564299 \\\\\n",
       "\tI(exper\\textasciicircum{}2) &   -0.7004939 &   -1.864158 \\\\\n",
       "\tage &  -30.5116345 &  -54.405012 \\\\\n",
       "\tkidslt6 & -442.0899082 & -894.021740 \\\\\n",
       "\tkidsge6 &  -32.7792266 &  -16.217996 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | linpartialeffects.coefficients | head.TobitRes.estimate...1. | \n",
       "|---|---|---|---|---|---|---|---|\n",
       "| (Intercept) | 1330.4824036 |  965.305296  | \n",
       "| nwifeinc |   -3.4466356 |   -8.814243  | \n",
       "| educ |   28.7611246 |   80.645605  | \n",
       "| exper |   65.6725131 |  131.564299  | \n",
       "| I(exper^2) |   -0.7004939 |   -1.864158  | \n",
       "| age |  -30.5116345 |  -54.405012  | \n",
       "| kidslt6 | -442.0899082 | -894.021740  | \n",
       "| kidsge6 |  -32.7792266 |  -16.217996  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "            linpartialeffects.coefficients head.TobitRes.estimate...1.\n",
       "(Intercept) 1330.4824036                    965.305296                \n",
       "nwifeinc      -3.4466356                     -8.814243                \n",
       "educ          28.7611246                     80.645605                \n",
       "exper         65.6725131                    131.564299                \n",
       "I(exper^2)    -0.7004939                     -1.864158                \n",
       "age          -30.5116345                    -54.405012                \n",
       "kidslt6     -442.0899082                   -894.021740                \n",
       "kidsge6      -32.7792266                    -16.217996                "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's print it in a single table for comparison\n",
    "data.frame(linpartialeffects$coefficients,head(TobitRes$estimate,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table has several noteworthy features. First, the Tobit coefficient estimates have the same sign as the corresponding OLS estimates. Second, though it is tempting to compare the magnitudes of the OLS and Tobit estimates, this is not very informative. We must be careful not to think that, because the Tobit coefficient on kidslt6 is roughly twice that of the OLS coefficient, the Tobit model implies a much greater response of hours worked to young children."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code computes APEs for all variables, where the APEs for the linear model are simply the OLS coefficients except for the variable exper , which appears as a quadratic. The Tobit APEs for nwifeinc , educ , and kidslt 6 are all substantially larger in magnitude than the corresponding OLS coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>linpartialeffects.coefficients..1.</th><th scope=col>margEff.TobitRes.</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>nwifeinc</th><td>  -3.4466356</td><td>  -5.326442 </td></tr>\n",
       "\t<tr><th scope=row>educ</th><td>  28.7611246</td><td>  48.734094 </td></tr>\n",
       "\t<tr><th scope=row>exper</th><td>  65.6725131</td><td>  79.504231 </td></tr>\n",
       "\t<tr><th scope=row>I(exper^2)</th><td>  -0.7004939</td><td>  -1.126509 </td></tr>\n",
       "\t<tr><th scope=row>age</th><td> -30.5116345</td><td> -32.876918 </td></tr>\n",
       "\t<tr><th scope=row>kidslt6</th><td>-442.0899082</td><td>-540.256832 </td></tr>\n",
       "\t<tr><th scope=row>kidsge6</th><td> -32.7792266</td><td>  -9.800526 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & linpartialeffects.coefficients..1. & margEff.TobitRes.\\\\\n",
       "\\hline\n",
       "\tnwifeinc &   -3.4466356 &   -5.326442 \\\\\n",
       "\teduc &   28.7611246 &   48.734094 \\\\\n",
       "\texper &   65.6725131 &   79.504231 \\\\\n",
       "\tI(exper\\textasciicircum{}2) &   -0.7004939 &   -1.126509 \\\\\n",
       "\tage &  -30.5116345 &  -32.876918 \\\\\n",
       "\tkidslt6 & -442.0899082 & -540.256832 \\\\\n",
       "\tkidsge6 &  -32.7792266 &   -9.800526 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | linpartialeffects.coefficients..1. | margEff.TobitRes. | \n",
       "|---|---|---|---|---|---|---|\n",
       "| nwifeinc |   -3.4466356 |   -5.326442  | \n",
       "| educ |   28.7611246 |   48.734094  | \n",
       "| exper |   65.6725131 |   79.504231  | \n",
       "| I(exper^2) |   -0.7004939 |   -1.126509  | \n",
       "| age |  -30.5116345 |  -32.876918  | \n",
       "| kidslt6 | -442.0899082 | -540.256832  | \n",
       "| kidsge6 |  -32.7792266 |   -9.800526  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "           linpartialeffects.coefficients..1. margEff.TobitRes.\n",
       "nwifeinc     -3.4466356                         -5.326442      \n",
       "educ         28.7611246                         48.734094      \n",
       "exper        65.6725131                         79.504231      \n",
       "I(exper^2)   -0.7004939                         -1.126509      \n",
       "age         -30.5116345                        -32.876918      \n",
       "kidslt6    -442.0899082                       -540.256832      \n",
       "kidsge6     -32.7792266                         -9.800526      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Partial Effects at the average x:\n",
    "data.frame(linpartialeffects$coefficients[-1],margEff(TobitRes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17-3. The Poisson Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another kind of nonnegative dependent variable is a count variable , which can take on nonnegative integer values: ${0,1,2,\\ldots}$. We are especially interested in cases where y takes on relatively few values, including zero. Examples include the number of children ever born to a woman, the number of times someone is arrested in a year, or the number of patents applied for by a firm in a year. For the same reasons discussed for binary and Tobit responses, a linear model for $E(y|x_1,\\ldots,x_k)$ might not provide the best fit over all values of the explanatory variables. (Nevertheless, it is always informative to start with a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with a Tobit outcome, we cannot take the logarithm of a count variable because it takes on the value zero. A profitable approach is to model the expected value as an exponential function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "E(y|x_1,x_2,\\ldots,x_k)=exp(\\beta_0+\\beta_1 x_1+\\ldots+\\beta_k x_k) \\tag{17.31}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although (17.31) is more complicated than a linear model, we basically already know how to interpret the coefficients. Taking the log of equation (17.31) shows that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "lod[E(y|x_1,x_2,\\ldots,x_k)]=\\beta_0+\\beta_1 x_1+\\ldots+\\beta_k x_k \\tag{17.31}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although (17.31) is more complicated than a linear model, we basically already know how to interpret the coefficients. $100\\beta_j$ is roughly the percentage change in $E(y|x)$ , given a one-unit increase in $x_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If, say, $x_j=log(z_j)$ for some variable $z_j > 0$, then its coefficient, $\\beta_j$ , is interpreted as an elasticity with respect to $z_j$. The bottom line is that, for practical purposes, we can interpret the coefficients in equation (17.31) as if we have a linear model, with $log(y)$ as the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A count variable cannot have a normal distribution (because the nor- mal distribution is for continuous variables that can take on all values), and if it takes on very few values, the distribution can be very different from normal. Instead, the nominal distribution for count data is the Poisson distribution ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are interested in the effect of explanatory variables on y , we must look at the Poisson distribution conditional on x . The Poisson distribution is entirely determined by its mean, so we only need to specify $E(y|x)$ . We assume this has the same form as (17.31), which we write in shorthand as $exp(x\\beta)$ . Then, the probability that y equals the value h , conditional on x , is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(y=h|x)=exp[-exp(x \\beta)][exp(x \\beta)]^h/h!, h=0,1,\\ldots,\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where h ! denotes factorial (see Appendix B). This distribution, which is the basis for the Poisson regression model , allows us to find conditional probabilities for any values of the explanatory variables. For example, P(y=0|x)=exp[-exp(x \\beta)] . Once we have estimates of the $\\beta_j$ , we can plug them into the probabilities for various values of x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the probit, logit, and Tobit models, we cannot directly compare the magnitudes of the Poisson estimates of an exponential function with the OLS estimates of a linear function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wooldridge Example 17.3 Poisson Regression for the number of Arrests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the Poisson regression model to the arrest data in CRIME1, used, among other places, in Example 9.1. The dependent variable, narr86 , is the number of times a man is arrested during 1986. This variable is zero for 1,970 of the 2,725 men in the sample, and only eight values of narr86 are greater than five. Thus, a Poisson regression model is more appropriate than a linear regression model. Table 17.5 also presents the results of OLS estimation of a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing package into '/home/nbuser/R'\n",
      "(as 'lib' is unspecified)\n"
     ]
    }
   ],
   "source": [
    "install.packages('stargazer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================================\n",
      "                 Dependent variable:     \n",
      "             ----------------------------\n",
      "                        narr86           \n",
      "                  OLS          Poisson   \n",
      "                  (1)            (2)     \n",
      "-----------------------------------------\n",
      "pcnv           -0.132***      -0.402***  \n",
      "                (0.040)        (0.085)   \n",
      "                                         \n",
      "avgsen           -0.011        -0.024    \n",
      "                (0.012)        (0.020)   \n",
      "                                         \n",
      "tottime          0.012         0.024*    \n",
      "                (0.009)        (0.015)   \n",
      "                                         \n",
      "ptime86        -0.041***      -0.099***  \n",
      "                (0.009)        (0.021)   \n",
      "                                         \n",
      "qemp86         -0.051***       -0.038    \n",
      "                (0.014)        (0.029)   \n",
      "                                         \n",
      "inc86          -0.001***      -0.008***  \n",
      "                (0.0003)       (0.001)   \n",
      "                                         \n",
      "black           0.327***      0.661***   \n",
      "                (0.045)        (0.074)   \n",
      "                                         \n",
      "hispan          0.194***      0.500***   \n",
      "                (0.040)        (0.074)   \n",
      "                                         \n",
      "born60           -0.022        -0.051    \n",
      "                (0.033)        (0.064)   \n",
      "                                         \n",
      "Constant        0.577***      -0.600***  \n",
      "                (0.038)        (0.067)   \n",
      "                                         \n",
      "-----------------------------------------\n",
      "Observations     2,725          2,725    \n",
      "=========================================\n",
      "Note:         *p<0.1; **p<0.05; ***p<0.01\n"
     ]
    }
   ],
   "source": [
    "library(foreign) ; library(stargazer) # package for regression output\n",
    "crime1 <- read.dta(\"https://github.com/thousandoaks/Wooldridge/blob/master/crime1.dta?raw=true\")\n",
    "\n",
    "# Estimate linear model\n",
    "lm.res      <-  lm(narr86~pcnv+avgsen+tottime+ptime86+qemp86+inc86+\n",
    "                    black+hispan+born60, data=crime1)\n",
    "# Estimate Poisson model\n",
    "Poisson.res <- glm(narr86~pcnv+avgsen+tottime+ptime86+qemp86+inc86+\n",
    "                    black+hispan+born60, data=crime1, family=poisson)\n",
    "\n",
    "stargazer(lm.res,Poisson.res,type=\"text\",keep.stat=\"n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OLS and Poisson coefficients are not directly comparable, and they have very different meanings. For example, the coefficient on pcnv implies that, if $\\Delta pcnv=.10$, the expected number of arrests falls by .013 ( pcnv is the proportion of prior arrests that led to conviction). The Poisson coefficient implies that $\\Delta pcnv=.10$ reduces expected arrests by about 4% [.402(.10)=.0402, and we multiply this by 100 to get the percentage effect]. As a policy matter, this suggests we can reduce overall arrests by about 4% if we can increase the probability of conviction by .1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Poisson coefficient on black implies that, other factors being equal, the expected number of arrests for a black man is estimated to be about 100 *[exp(.661)-1]= 93.7% higher than for a white man with the same values for the other explanatory variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17-4. Censored and Truncated Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models in Sections 17-1, 17-2, and 17-3 apply to various kinds of limited dependent variables that arise frequently in applied econometric work. In using these methods, it is important to remember that we use a probit or logit model for a binary response, a Tobit model for a corner solution out- come, or a Poisson regression model for a count response because we want models that account for important features of the distribution of y . There is no issue of data observability. For example, in the Tobit application to women's labor supply in Example 17.2, there is no problem with observing hours worked: it is simply the case that a nontrivial fraction of married women in the population choose not to work for a wage. In the Poisson regression application to annual arrests, we observe the dependent variable for every young man in a random sample from the population, but the dependent variable can be zero as well as other small integer values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the distinction between lumpiness in an outcome variable (such as taking on the value zero for a nontrivial fraction of the population) and problems of data censoring can be confus- ing. This is particularly true when applying the Tobit model. In this book, the standard Tobit model described in Section 17-2 is only for corner solution outcomes. But the literature on Tobit models usually treats another situation within the same framework: the response variable has been censored above or below some threshold. Typically, the censoring is due to survey design and, in some cases, institutional constraints. Rather than treat data censoring problems along with corner solution out- comes, we solve data censoring by applying a censored regression model . Essentially, the problem solved by a censored regression model is one of missing data on the response variable, y . Although we are able to randomly draw units from the population and obtain information on the explanatory vari- ables for all units, the outcome on y i is missing for some i . Still, we know whether the missing values are above or below a given threshold, and this knowledge provides useful information for estimating the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A truncated regression model arises when we exclude, on the basis of y , a subset of the population in our sampling scheme. In other words, we do not have a random sample from the underlying population, but we know the rule that was used to include units in the sample. This rule is determined by whether y is above or below a certain threshold. We explain more fully the difference between censored and truncated regression models later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.4a Censored Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While censored regression models can be defined without distributional assumptions, in this subsection we study the censored normal regression model . The variable we would like to explain, y , follows the classical linear model.But rather than observing $y_i$ , we observe it only if it is less than a censoring value, $c_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example of right data censoring is top coding . When a variable is top coded, we know its value only up to a certain threshold. For responses greater than the threshold, we only know that the variable is at least as large as the threshold. For example, in some surveys family wealth is top coded. Suppose that respondents are asked their wealth, but people are allowed to respond with \"more than $500,000.\" Then, we observe actual wealth for those respondents whose wealth is less than $500,000 but not for those whose wealth is greater than $500,000. In this case, the censoring threshold, $c_i$ , is the same for all i . In many situations, the censoring threshold changes with individual or family characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we observed a random sample for (x,y) , we would simply estimate b by OLS, and statistical inference would be standard. (We again absorb the intercept into x for simplicity.) The censoring causes problems. Using arguments similar to the Tobit model, an OLS regression using only the uncensored observations -that is, those with $y_i<c_i$ - produces inconsistent estimators of the $\\beta_j$ . An OLS regression of $w_i$ on $x_i$ , using all observations, does not consistently estimate the $b_j$ , unless there is no censoring. This is similar to the Tobit case, but the problem is much different. In the Tobit model, we are modeling economic behavior, which often yields zero outcomes; the Tobit model is supposed to reflect this. With censored regression, we have a data collection problem because, for some reason, the data are censored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to know that we can interpret the $b_j$ just as in a linear regression model under random sampling. This is much different than Tobit applications to corner solution responses, where the expectations of interest are nonlinear functions of the $\\beta_j$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important application of censored regression models is duration analysis . A duration is a variable that measures the time before a certain event occurs. For example, we might wish to explain the number of days before a felon released from prison is arrested. For some felons, this may never happen, or it may happen after such a long time that we must censor the duration in order to analyze the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wooldridge Example 17.4 Duration of Recidivism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file RECID contains data on the time in months until an inmate in a North Carolina prison is arrested after being released from prison; call this durat . Some inmates participated in a work program while in prison. We also control for a variety of demographic variables, as well as for measures of prison and criminal history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of 1,445 inmates, 893 had not been arrested during the period they were followed; therefore, these observations are censored. The censoring times differed among inmates, ranging from 70 to 81 months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, it is crucial to account for the censoring, especially because almost 62% of the durations are censored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code computes a censored normal regression for log(durat). Each of the coefficients, when multiplied by 100, gives the estimated percentage change in expected duration, given a ceteris paribus increase of one unit in the corresponding explanatory variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "survreg(formula = Surv(log(durat), uncensored, type = \"right\") ~ \n",
       "    workprg + priors + tserved + felon + alcohol + drugs + black + \n",
       "        married + educ + age, data = recid, dist = \"gaussian\")\n",
       "               Value Std. Error      z        p\n",
       "(Intercept)  4.09939   0.347535 11.796 4.11e-32\n",
       "workprg     -0.06257   0.120037 -0.521 6.02e-01\n",
       "priors      -0.13725   0.021459 -6.396 1.59e-10\n",
       "tserved     -0.01933   0.002978 -6.491 8.51e-11\n",
       "felon        0.44399   0.145087  3.060 2.21e-03\n",
       "alcohol     -0.63491   0.144217 -4.402 1.07e-05\n",
       "drugs       -0.29816   0.132736 -2.246 2.47e-02\n",
       "black       -0.54272   0.117443 -4.621 3.82e-06\n",
       "married      0.34068   0.139843  2.436 1.48e-02\n",
       "educ         0.02292   0.025397  0.902 3.67e-01\n",
       "age          0.00391   0.000606  6.450 1.12e-10\n",
       "Log(scale)   0.59359   0.034412 17.249 1.13e-66\n",
       "\n",
       "Scale= 1.81 \n",
       "\n",
       "Gaussian distribution\n",
       "Loglik(model)= -1597.1   Loglik(intercept only)= -1680.4\n",
       "\tChisq= 166.74 on 10 degrees of freedom, p= 0 \n",
       "Number of Newton-Raphson Iterations: 4 \n",
       "n= 1445 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(foreign);library(survival)\n",
    "recid <- read.dta(\"https://github.com/thousandoaks/Wooldridge/blob/master/recid.dta?raw=true\")\n",
    "\n",
    "# Define Dummy for UNcensored observations\n",
    "recid$uncensored <- recid$cens==0\n",
    "# Estimate censored regression model:\n",
    "res<-survreg(Surv(log(durat),uncensored, type=\"right\") ~ workprg+priors+\n",
    "                     tserved+felon+alcohol+drugs+black+married+educ+age, \n",
    "                     data=recid, dist=\"gaussian\")\n",
    "# Output:\n",
    "summary(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several of the coefficients are interesting. The variables priors (number of prior convictions) and tserved (total months spent in prison) have negative effects on the time until the next arrest occurs. This suggests that these variables measure proclivity for criminal activity rather than representing a deterrent effect. For example, an inmate with one more prior conviction has a duration until next arrest that is almost 14% less. A year of time served reduces duration by about $100*12(.019)=22.8%$. A somewhat surprising finding is that a man serving time for a felony has an estimated expected duration that is almost 56% [exp(.444)-1=.56%] longer than a man serving time for a nonfelony."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those with a history of drug or alcohol abuse have substantially shorter expected durations until the next arrest. (The variables alcohol and drugs are binary variables.) Older men, and men who were married at the time of incarceration, are expected to have significantly longer durations until their next arrest. Black men have substantially shorter durations, on the order of 42% [exp(.543)-1 =-.42]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key policy variable, workprg , does not have the desired effect. The point estimate is that, other things being equal, men who participated in the work program have estimated recidivism durations that are about 6.3% shorter than men who did not participate. The coefficient has a small t statistic, so we would probably conclude that the work program has no effect. This could be due to a self-selection problem, or it could be a product of the way men were assigned to the program. Of course, it may simply be that the program was ineffective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.4b Truncated Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The truncated regression model differs in an important respect from the censored regression model. In the case of data censoring, we do randomly sample units from the population. The censoring problem is that, while we always observe the explanatory variables for each randomly drawn unit, we observe the outcome on y only when it is not censored above or below a given threshold. With data truncation, we restrict attention to a subset of the population prior to sampling; so there is a part of the popula- tion for which we observe no information. In particular, we have no information on explanatory vari- ables. The truncated sampling scenario typically arises when a survey targets a particular subset of the population and, perhaps due to cost considerations, entirely ignores the other part of the population. Subsequently, researchers might want to use the truncated sample to answer questions about the entire population, but one must recognize that the sampling scheme did not generate a random sample from the whole population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, Hausman and Wise (1977) used data from a negative income tax experiment to study various determinants of earnings. To be included in the study, a family had to have income less than 1.5 times the 1967 poverty line, where the poverty line depended on family size. Hausman and Wise wanted to use the data to estimate an earnings equation for the entire population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The truncated normal regression model begins with an underlying population model that satis- fies the classical linear model assumptions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "y=\\beta_0+x*\\beta+u,u|x \\sim Normal(0,\\sigma^2) \\tag{17.40}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that this is a strong set of assumptions, because u must not only be independent of x , but also normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under (17.40) we know that, given a random sample from the population, OLS is the most efficient estimation procedure. The problem arises because we do not observe a random sample from the population: Assumption MLR.2 is violated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "r"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
