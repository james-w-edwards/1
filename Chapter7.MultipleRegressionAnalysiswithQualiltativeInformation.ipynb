{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. Multiple Regression Analysis with Qualitative Information: Binary (or Dummy) Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous chapters, the dependent and independent variables in our multiple regression models have had quantitative meaning. In empirical work, we must also incorporate qualitative factors into regression models. The gender or race of an individual, the industry of a firm or the region where a city is located are all considered to be qualitative factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-1 Describing qualitative information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qualitative factors often come in the form of binary information: a person is female or male, a person does or does not have a personal computer, a state administers capital punishment or not. In all these examples this information can be captured by defining a binary variable or a zero-one variable. In econometrics binary variables are most commonly called dummy variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In assigning a dummy variable we must decide which event is assigned the value one and which is assigned the value zero. For example, in a study of individual wage determination, we might define female to be a binary variable taling on the value one for females and the value zero for males.The name in this case indicates the event with the value one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-2 A Single Dummy Independent Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we incorporate binary information into regression models ? In the simplest case with only a single dummy explanatory variable, we just add it as an independent variable in the equation. For example, consider the following simple model of hourly wage determination:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "wage=\\beta_0+\\delta_0female+\\beta_1educ+u\n",
    "\\tag{7.1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In model (7.1), only two observed factors affect wage: gender and education. Because female=1 when the person is female, and female=0 when the person is male, the parameter $\\deta_0$ has the following interpretation: $\\delta_0$ is the difference in hourly wage between females and males, given the same amount of education (and the same error term u). Thus the coefficient $\\delta_0$ determines whether there is discrimination againg women: if $\\delta_0\\leq0$ then, for the same level of other factors, women earn less on average than men."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of expectations, if we assume the zero conditional mean assumption e(u|female,educ)=0, then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\delta_0=E(wage|female,educ)-E(wage|male,educ)\n",
    "\\tag{7.2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key here is that the level of education is the same in both expectations; the difference $\\delta_0$ is due to gender only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In (7.1) we have chosen males to be the base group or benchmark group, that is the group against comparisons are made. This is why $\\beta_0$ is the intercept for males and $\\delta_0$ is the difference in intercepts between females and males."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wooldridge. Example 7.1 Hourly Wage Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the data in WAGE1, we estimate the following model (7.3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "wage=\\beta_0+\\delta_0*female+\\beta_1*female+\\beta_2*exper+\\beta_3*tenure+u\n",
    "\\tag{7.3}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If educ, exper and tenure all relevant productivity characteristics, the null hypothesis of not difference between men and women is $H_0:\\delta_0=0$. The alternative that there is discrimination against women is $H_1:\\delta_0\\leq0$. Following we compute the model by OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(foreign)\n",
    "wage1 <- read.dta(\"https://github.com/thousandoaks/Wooldridge/blob/master/wage1.dta?raw=true\")\n",
    "\n",
    "wageres <- lm(wage ~ female+educ+exper+tenure, data=wage1)\n",
    "summary(wageres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient on female is interesting because it measures the average difference in hourly wage between a man and a woman who have the same levels of educ, exper and tenure. If we take a woman and a man with the same levels of education, experience and tenure the woman earns, on average, $1.81 less per hour than the man (in 1976 wages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wooldridge. Example 7.2 Effects of Computer Ownership on College GPA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to determine the effects of computer ownership on college grade point average, we estimate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "colGPA=\\beta_0+\\delta_0*PC+\\beta_1*hsGPA+\\beta_2*ACT+u\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where the dummy variable PC equals one if a student owns a personal computer and zero otherwise. The variables hsGPA (high school GPA) and ACT (achievent test score) are used as controls: it could be that stronger students, as measured by high school GPA and ACT scores, are more likely to own computers. we control for these factors because we would like to know the average effect on colGPA is a student is picked at random and given a personal computer. Using the data is GPA1, we obtain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(foreign)\n",
    "gpa1 <- read.dta(\"https://github.com/thousandoaks/Wooldridge/blob/master/gpa1.dta?raw=true\")\n",
    "\n",
    "# Store results under \"GPAres\" and display full table:\n",
    "GPAres <- lm(colGPA ~ PC+hsGPA+ACT, data=gpa1)\n",
    "summary(GPAres)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equation implies that a student who owns a PC has a predicted GPA about .16 points higher than a comparable student without a PC. The effect is also very statistically significant, with $t_{PC}=.157/.057\\approx 2.75$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the previous examples can be viewed as having relevance for policy analysis. In the first example, we were interested in gender discrimination in the workforce. In the second example we were concerned with the effect of computer ownership on college performance. A special case of policy analysis is program evaluation, where we would like to know the effect of economic or social programs on individuals, firms, neighborhoods, cities and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simplest case, there are two groups of subjects. The control group does not participate in the program. The experimental group or treatment group does take part in the program. Except in rare cases the choice of the control and the treatment groups is not random. However, in some cases, multiple regression analysis can be used to control for enough other factors in order to estimate the causal effect of the program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wooldridge. Example 7.3 Effects of Training Grants on Hours of Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the 1988 data for Michigan manufacturing firms in JTRAIN, we obtain the following estimated equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(foreign)\n",
    "jtrain <- read.dta(\"https://github.com/thousandoaks/Wooldridge/blob/master/jtrain.dta?raw=true\")\n",
    "\n",
    "# Store results under \"GPAres\" and display full table:\n",
    "TRAINres <- lm(hrsemp ~ grant+log(sales)+log(employ), data=jtrain[jtrain$year == 1988,])\n",
    "summary(TRAINres)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dependent variable is hours of training per employee, at the firm level. The variable grant is a dummy variable equal to one if the firm received a job training grant for 1988, and zero otherwise. The variables sales and employ represent annual sales and number of employees respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable grant is very statistically significant, with $t_grant=4.70$. Controlling for sales and employment, firms that received a grant trained each worker, on average, 26.25 hours more. The coefficient on log(sales) is small and very insignificant. The coefficient on log(employ) means that, if a firm is 10% larger, it trains its workers about .61 hours less. Its t statistic is -1.56, which is only marginally statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with any other independent variable, we should ask whether the measured effect of a qualitative variable is causal. In the previous model, is the difference in training between firms that receive grants and those who do not due to the grant, or is grant receipt simply an indicator of something else ? It might be that the firms receiving grants would have, on average, trained their workers more even in the absence of a grant. Nothing in this analysis tells us whether we have estimated a causal effect; we must know how the firms receiving grants were determined. We can only hope we have controlled for as many factors as possible that might be related to whether a firm received a grand and to its levels of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-2a Interpreting Coefficients on Dummy Explanatory Variables When the Dependent Variable is log(y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common specification in applied work has the dependent variable appearing in logarithmic form, with one or more dummy variables appearing as independent variables. In this case the coefficients have a percentage interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wooldridge. Example 7.4 Housing Price Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the data in HPRICE1, we obtain the equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "library(foreign)\n",
    "hprice <- read.dta(\"https://github.com/thousandoaks/Wooldridge/blob/master/hprice1.dta?raw=true\")\n",
    "\n",
    "# Store results under \"GPAres\" and display full table:\n",
    "HPRICEres <- lm(log(price) ~ log(lotsize)+log(sqrft)+bdrms+colonial, hprice)\n",
    "summary(HPRICEres)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the variables are explanatory except colonial, which is a binary variable equal to one if the house is of the colonial style. For given levels of lotsize, sqrft and bdrms, the difference in log(price) between a house of colonial style and that of another style is .054. This means that a colonial-style house is predicted to sell for about 5.4% more, holding other factors fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wooldridge. Example 7.5 Log Hourly Wage Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us reestimate the wage equation from Example 7.1, using log(wage) as the dependent variable and adding quadratics on exper and tenure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = log(wage) ~ female + educ + exper + I(exper^2) + \n",
       "    tenure + I(tenure^2), data = wage1)\n",
       "\n",
       "Residuals:\n",
       "     Min       1Q   Median       3Q      Max \n",
       "-1.83160 -0.25658 -0.02126  0.25500  1.13370 \n",
       "\n",
       "Coefficients:\n",
       "              Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  0.4166910  0.0989279   4.212 2.98e-05 ***\n",
       "female      -0.2965110  0.0358054  -8.281 1.04e-15 ***\n",
       "educ         0.0801966  0.0067573  11.868  < 2e-16 ***\n",
       "exper        0.0294324  0.0049752   5.916 6.00e-09 ***\n",
       "I(exper^2)  -0.0005827  0.0001073  -5.431 8.65e-08 ***\n",
       "tenure       0.0317139  0.0068452   4.633 4.56e-06 ***\n",
       "I(tenure^2) -0.0005852  0.0002347  -2.493    0.013 *  \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 0.3998 on 519 degrees of freedom\n",
       "Multiple R-squared:  0.4408,\tAdjusted R-squared:  0.4343 \n",
       "F-statistic: 68.18 on 6 and 519 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(foreign)\n",
    "wage1 <- read.dta(\"https://github.com/thousandoaks/Wooldridge/blob/master/wage1.dta?raw=true\")\n",
    "\n",
    "wageres <- lm(log(wage) ~ female+educ+exper+I(exper^2)+tenure+I(tenure^2), data=wage1)\n",
    "summary(wageres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore the proportionate difference in wages between females and males, holding other factors fixed is: $(\\hat{wage_F}-\\hat{wage_M})/\\hat{wage_M}$. Based on the previous model we have: $\\hat{log(wage_F)}-\\hat{log(wage_M)}=-.297$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponentiating and substracting one gives: $\\hat{wage_F}-\\hat{wage_M}/\\hat{wage_M}=exp(-.297)-1 \\approx -.257$. Remeber that $log(wage_F)-log(wage_M)=log(wage_F/wage_M)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This estimate implies that a woman's wage is, on average, 25.7% below a comparable man's wage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-3 Using Dummy Variables for Multiple Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use several dummy independent variables in the same equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wooldridge. Example 7.6 Log Hourly Wage Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us estimate a model that allows for wage differences among four groups: married men, married women, single men, and single women. To do this we must select a base group; we choose single men. Then, we must define dummy variables for each of the remaining groups. Call these marrmale, marrfem and singfem. We drop female as it is now redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = lwage ~ marrmale + marrfem + singfem + educ + exper + \n",
       "    I(exper^2) + tenure + I(tenure^2), data = wage1)\n",
       "\n",
       "Residuals:\n",
       "     Min       1Q   Median       3Q      Max \n",
       "-1.89697 -0.24060 -0.02689  0.23144  1.09197 \n",
       "\n",
       "Coefficients:\n",
       "              Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  0.3213780  0.1000090   3.213 0.001393 ** \n",
       "marrmale     0.2126756  0.0553572   3.842 0.000137 ***\n",
       "marrfem     -0.1982676  0.0578355  -3.428 0.000656 ***\n",
       "singfem     -0.1103502  0.0557421  -1.980 0.048272 *  \n",
       "educ         0.0789103  0.0066945  11.787  < 2e-16 ***\n",
       "exper        0.0268006  0.0052428   5.112 4.50e-07 ***\n",
       "I(exper^2)  -0.0005352  0.0001104  -4.847 1.66e-06 ***\n",
       "tenure       0.0290875  0.0067620   4.302 2.03e-05 ***\n",
       "I(tenure^2) -0.0005331  0.0002312  -2.306 0.021531 *  \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 0.3933 on 517 degrees of freedom\n",
       "Multiple R-squared:  0.4609,\tAdjusted R-squared:  0.4525 \n",
       "F-statistic: 55.25 on 8 and 517 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(foreign)\n",
    "wage1 <- read.dta(\"https://github.com/thousandoaks/Wooldridge/blob/master/wage1.dta?raw=true\")\n",
    "\n",
    "# Example 7.6\n",
    "  # Generate the subgroup dummies\n",
    "\n",
    "marrmale<-as.numeric(wage1$female==0 & wage1$married==1)\n",
    "marrfem<-as.numeric(wage1$female==1 & wage1$married==1)\n",
    "singfem<-as.numeric(wage1$female==1 & wage1$married==0)\n",
    " \n",
    "wageres<-lm(lwage ~ marrmale + marrfem + singfem + educ + exper + \n",
    "             I(exper^2) + tenure + I(tenure^2), data=wage1)\n",
    "summary(wageres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the coefficients, with the exception of singfem (single female), have t statistics well above two in absolute value. The t statistic for singfem is about -1.98, which is just significant at the 5% level agains a two-sided alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interpret the coefficients on the dummy variables, we must remember that the base group is single males. Thus the estimates on the three dummy variables measure the proportionate difference in wage relative to single males. For example, married men (marrmale) are estimated to earn about 21.3% more than single men (the base group), holding all levels of education, experience and tenure fixed. The more precise estimate as shown previously is about 23.7%. A married woman (marrfem), on the other hand, earns a predicted 19.8% less than a single man (the base group) with the same levels of the other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the base group is represented by the intercept, we have included dummy variables for only three of the four groups. I we were to add a dummy variable for single males to the model we would fall into the dummy variable trap by introducing perfect collinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the previous model to obtain the estimated difference between any two groups. The estimated proportionate difference between single and married women is -.110 - (-.198)=.088, which means that single women earn about 8.8% more than married women."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately we cannot use the model for testing whether the estimated difference between single and married women is statistically significant. Knowing the standard errors on marrfem and singfem is not enough to carry out the test (refer to Section 4-4). The easiest thing to do is to choose one of these groups to be the base group and to reestimate the equation. Nothing substantive changes, but we get the neeeded estimate and its standard error directly. When we use married women as the base group, we obtain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = lwage ~ marrmale + singmale + singfem + educ + exper + \n",
       "    I(exper^2) + tenure + I(tenure^2), data = wage1)\n",
       "\n",
       "Residuals:\n",
       "     Min       1Q   Median       3Q      Max \n",
       "-1.89697 -0.24060 -0.02689  0.23144  1.09197 \n",
       "\n",
       "Coefficients:\n",
       "              Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  0.1231104  0.1057937   1.164 0.245089    \n",
       "marrmale     0.4109433  0.0457709   8.978  < 2e-16 ***\n",
       "singmale     0.1982676  0.0578355   3.428 0.000656 ***\n",
       "singfem      0.0879174  0.0523481   1.679 0.093664 .  \n",
       "educ         0.0789103  0.0066945  11.787  < 2e-16 ***\n",
       "exper        0.0268006  0.0052428   5.112 4.50e-07 ***\n",
       "I(exper^2)  -0.0005352  0.0001104  -4.847 1.66e-06 ***\n",
       "tenure       0.0290875  0.0067620   4.302 2.03e-05 ***\n",
       "I(tenure^2) -0.0005331  0.0002312  -2.306 0.021531 *  \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 0.3933 on 517 degrees of freedom\n",
       "Multiple R-squared:  0.4609,\tAdjusted R-squared:  0.4525 \n",
       "F-statistic: 55.25 on 8 and 517 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "library(foreign)\n",
    "wage1 <- read.dta(\"https://github.com/thousandoaks/Wooldridge/blob/master/wage1.dta?raw=true\")\n",
    "\n",
    "# Example 7.6\n",
    "  # Generate the subgroup dummies\n",
    "\n",
    "marrmale<-as.numeric(wage1$female==0 & wage1$married==1)\n",
    "singmale<-as.numeric(wage1$female==0)*as.numeric(wage1$married==0)\n",
    "singfem<-as.numeric(wage1$female==1 & wage1$married==0)\n",
    " \n",
    "wageres<-lm(lwage ~ marrmale + singmale + singfem + educ + exper + \n",
    "             I(exper^2) + tenure + I(tenure^2), data=wage1)\n",
    "summary(wageres)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimate on singfem (single female) is, as expected, .088. Now, we have a standard error to go along with this estimate. The t statistic for the null that there is no difference in the population between married women (base group) and single women (singfem) is 1.68. This is marginal evidence against the null hypothesis. We also see that the estimated difference between married men (marrmale) and married women (the base group) is very statistically significant 8.98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-3a Incorporating Ordinal Information by Using Dummy Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we would like to estimate the effect of city credit ratings on the municipal bond interest rate (MBR). Several financial companies, such as Moody&#8217;s Investors Service and Standard and Poor&#8217;s, rate the quality of debt for local governments, where the ratings depend on things like probability of default. (Local governments prefer lower interest rates in order to reduce their costs of borrowing). For simplicity, suppose that rankings take on the integer values {0, 1, 2, 3, 4}, with zero being the worst credit rating and four being the best. This is an example of an ordinal variable. Call this CR for concreteness. The question we need to address is: How do we incorporate the variable CR into a model to explain MBR ?. One possibility is to just include CR as we would include any other explanatory variable:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$MBR=\\beta_0+\\beta_1*CR+others$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A viable approach, given that CR takes on relatively few values, is to define dummy variables for each value of CR. Thus, let $CR_1=1$ if $CR=1$ and $CR_1=0$ otherwise. $CR_2=1$ if $CR=2$ and $CR_2=0$ otherwise; and so on. Effectively, we take the single credit rating and turn it into five categories. Then, we can estimate the mode;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "MBR=\\beta_0+\\delta_1*CR_1+\\delta_2*CR_2+\\delta_3*CR_3+\\delta_4*CR_4+others\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following our rule for including dummy variables in a model, we include four dummy variables because we have five categories. The ommitted category here is the credit rating of zero, and so it is the base group. The coefficients are easy to interpret: $\\delta_1$ is the difference in MBR (other factors fixed) between a municipality with a credit of one and a municipality with a credir rating of zero, $\\delta_2$ is the difference in MBR between a municipality with a credit rating of two and a municipality with a credit rating of zero; and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wooldridge. Example 7.8 Effects of Law School Rankings on Starting Salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the file LAWSCH85 contains data on median starting salaries for school graduates. One of the key explanatory variables is the rank of the law school. Because each law school has a different rank, we clearly cannot include a dummy variable for each rank. If we do not wish to put the rank directly in the equation, we can break it down into categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the dummy variables top10, r11_25, r26_40,r41_60,r61_100 to take on the value unity when the variable rank falls into the appropriate range. We let schools ranked below 100 be the base group. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a numeric variable, we need to generate a categorical (factor) variable to represent the range into which the rank of a school falls. In R, the command cut is very convenient for this. It takes a numeric variable and a vector of cut points and returns a factor variable. By default, the upper cut and points are included in the corresponding range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "   (0,10]   (10,25]   (25,40]   (40,60]  (60,100] (100,175] \n",
       "       10        16        13        18        37        62 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = log(salary) ~ rankcat + LSAT + GPA + log(libvol) + \n",
       "    log(cost), data = lawsch85)\n",
       "\n",
       "Residuals:\n",
       "      Min        1Q    Median        3Q       Max \n",
       "-0.294888 -0.039691 -0.001682  0.043888  0.277497 \n",
       "\n",
       "Coefficients:\n",
       "                 Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)     9.1652952  0.4114243  22.277  < 2e-16 ***\n",
       "rankcat(0,10]   0.6995659  0.0534919  13.078  < 2e-16 ***\n",
       "rankcat(10,25]  0.5935434  0.0394400  15.049  < 2e-16 ***\n",
       "rankcat(25,40]  0.3750763  0.0340812  11.005  < 2e-16 ***\n",
       "rankcat(40,60]  0.2628191  0.0279621   9.399 3.18e-16 ***\n",
       "rankcat(60,100] 0.1315950  0.0210419   6.254 5.71e-09 ***\n",
       "LSAT            0.0056908  0.0030630   1.858   0.0655 .  \n",
       "GPA             0.0137255  0.0741919   0.185   0.8535    \n",
       "log(libvol)     0.0363619  0.0260165   1.398   0.1647    \n",
       "log(cost)       0.0008412  0.0251360   0.033   0.9734    \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 0.08564 on 126 degrees of freedom\n",
       "  (20 observations deleted due to missingness)\n",
       "Multiple R-squared:  0.9109,\tAdjusted R-squared:  0.9046 \n",
       "F-statistic: 143.2 on 9 and 126 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(foreign)\n",
    "lawsch85<-\n",
    "     read.dta(\"https://github.com/thousandoaks/Wooldridge/blob/master/lawsch85.dta?raw=true\")\n",
    "\n",
    "# Define cut points for the rank\n",
    "cutpts <- c(0,10,25,40,60,100,175)\n",
    "\n",
    "# Create factor variable containing ranges for the rank\n",
    "lawsch85$rankcat <- cut(lawsch85$rank, cutpts)\n",
    "\n",
    "# Display frequencies\n",
    "table(lawsch85$rankcat)\n",
    "\n",
    "# Choose reference category\n",
    "lawsch85$rankcat <- relevel(lawsch85$rankcat,\"(100,175]\")\n",
    "\n",
    "# Run regression\n",
    "reg<-lm(log(salary)~rankcat+LSAT+GPA+log(libvol)+log(cost), data=lawsch85)\n",
    "summary(reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that all of the dummy variables defining the different ranks are very statistically significant. The estimate rankcat(60_100] means that, holding LSAT, GPA, libvol and cost fixed, the median salary at a law school ranked between 61 and 100 is about 13.2% higher than that at a law school ranked below 100. The difference between a top 10 school and a below 100 school is quite large. Using the exact calculation given in equation (7.10) gives $exp(.700)-1 \\approx 1.014$, and so the predicted median salary is more than 100% higher at a top 10 school than it is at a below 100 school."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-4 Interactions Involving Dummy Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-4b Allowing for different slopes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as variables with quantitative meaning can be interacted in regression models, so can dummy variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also occasions for interacting dummy variables with explanatory variables that are not dummy variables to allow for a difference in slopes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wooldridge. Example 7.10 Log Hourly Wage Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing with the wage example, suppose that we wish to test whether the return to education is the same for men and women, allowing for a constant wage differential between men and women (a differential for which we have already found evidence). For simplicity, we include only education and gender in the model. To apply OLS, we must write the model with an interaction between female and educ:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "log(wage)=\\beta_0+\\delta_0*female+\\beta_1*educ+\\delta_1*female*educ+u\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important hypothesis is that the return to education is the same for women and men. \n",
    "In terms of the previous model, this is stated as $H_0: \\delta_1=0$, which means that the slope of log( wage ) with respect to educ is the same for men and women. Note that this hypothesis puts no restrictions on the difference in intercepts, $\\delta_0$. A wage differential between men and women is allowed under this null, but it must be the same at all levels of education."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = log(wage) ~ female + educ + exper + female * educ + \n",
       "    exper + I(exper^2) + tenure + I(tenure^2), data = wage1)\n",
       "\n",
       "Residuals:\n",
       "     Min       1Q   Median       3Q      Max \n",
       "-1.83265 -0.25261 -0.02374  0.25396  1.13584 \n",
       "\n",
       "Coefficients:\n",
       "              Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  0.3888060  0.1186871   3.276  0.00112 ** \n",
       "female      -0.2267886  0.1675394  -1.354  0.17644    \n",
       "educ         0.0823692  0.0084699   9.725  < 2e-16 ***\n",
       "exper        0.0293366  0.0049842   5.886 7.11e-09 ***\n",
       "I(exper^2)  -0.0005804  0.0001075  -5.398 1.03e-07 ***\n",
       "tenure       0.0318967  0.0068640   4.647 4.28e-06 ***\n",
       "I(tenure^2) -0.0005900  0.0002352  -2.509  0.01242 *  \n",
       "female:educ -0.0055645  0.0130618  -0.426  0.67028    \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 0.4001 on 518 degrees of freedom\n",
       "Multiple R-squared:  0.441,\tAdjusted R-squared:  0.4334 \n",
       "F-statistic: 58.37 on 7 and 518 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(foreign)\n",
    "wage1 <- read.dta(\"http://fmwww.bc.edu/ec-p/data/wooldridge/wage1.dta\")\n",
    "\n",
    "wagereg<-(lm(log(wage)~female+educ+exper+female*educ+exper+I(exper^2)+tenure+I(tenure^2),\n",
    "                                                           data=wage1))\n",
    "summary(wagereg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated return to education for men in this equation is .082, or 8.2%. For women, it is .082-.0055=0764, or about 7.6%. The difference, -.56%, or just over one-half a percent- age point less for women, is not economically large nor statistically significant: the t statistic is -0.426. Thus, we conclude that there is no evidence against the hypothesis that the return to education is the same for men and women."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient on female , while remaining economically large, is no longer significant at conventional levels (t=-1.35) . Its coefficient and t statistic in the equation without the interaction were -0.29 and -8.28, respectively [see example (7.5)]. Should we now conclude that there is no statistically significant evidence of lower pay for women at the same levels of educ , exper , and tenure ? This would be a serious error. Because we have added the interaction female*educ to the equation, the coef- ficient on female is now estimated much less precisely than it was in example (7.5): the standard error has increased by almost fivefold (.167 /.035=4.77) . This occurs because female and female*educ are highly correlated in the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, there is a useful way to think about the multicollinearity: in the previous model a, $\\delta_0$ measures the wage differential between women and men when educ=0. Very few people in the sample have very low levels of education, so it is not surprising that we have a difficult time estimating the differential at educ=0 (nor is the differential at zero years of education very informative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-4c Testing for Differences in Regression Functions accross Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous examples illustrate that interacting dummy variables with other independent variables can be a powerful tool. Sometimes, we wish to test the null hypothesis that two populations or groups follow the same regression function, against the alternative that one or more of the slopes differ across the groups. We will also see examples of this in Chapter 13, when we discuss pooling different cross sections over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to test whether the same regression model describes college grade point aver- ages for male and female college athletes. The equation is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "cumgpa=\\beta_0+\\beta_1*sat+\\beta_2*hsperc+\\beta_3*tothrs+u\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where sat is SAT score, hsperc is high school rank percentile, and tothrs is total hours of college courses. We know that, to allow for an intercept difference, we can include a dummy variable for either males or females. If we want any of the slopes to depend on gender, we simply interact the appropriate variable with, say, female , and include it in the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are interested in testing whether there is any difference between men and women, then we must allow a model where the intercept and all slopes can be different across the two groups:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "cumgpa=\\beta_0+\\delta_0*female+\\beta_1*sat+\\delta_1*female*sat+\\beta_2*hsperc+\\delta_2*female*hsperc+\\beta_3*tothrs+\\delta_3*female*tothrs+u\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter $\\delta_0$ is the difference in the intercept between women and men, $\\delta_1$ is the slope difference with respect to sat between women and men, and so on. The null hypothesis that cumgpa follows the same model for males and females is stated as: $ \\H_0:\\delta_0=0, \\delta_1=0,\\delta_2=0,\\delta_3=0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one of the $\\delta_j$ is different from zero, then the model is different from men and women. Using the spring semester data from the file GPA3, the full model is estimated as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = cumgpa ~ female * (sat + hsperc + tothrs), data = gpa3, \n",
       "    subset = (spring == 1))\n",
       "\n",
       "Residuals:\n",
       "     Min       1Q   Median       3Q      Max \n",
       "-1.51370 -0.28645 -0.02306  0.27555  1.24760 \n",
       "\n",
       "Coefficients:\n",
       "                Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)    1.4808117  0.2073336   7.142 5.17e-12 ***\n",
       "female        -0.3534862  0.4105293  -0.861  0.38979    \n",
       "sat            0.0010516  0.0001811   5.807 1.40e-08 ***\n",
       "hsperc        -0.0084516  0.0013704  -6.167 1.88e-09 ***\n",
       "tothrs         0.0023441  0.0008624   2.718  0.00688 ** \n",
       "female:sat     0.0007506  0.0003852   1.949  0.05211 .  \n",
       "female:hsperc -0.0005498  0.0031617  -0.174  0.86206    \n",
       "female:tothrs -0.0001158  0.0016277  -0.071  0.94331    \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 0.4678 on 358 degrees of freedom\n",
       "Multiple R-squared:  0.4059,\tAdjusted R-squared:  0.3943 \n",
       "F-statistic: 34.95 on 7 and 358 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(foreign)\n",
    "gpa3 <- read.dta(\"http://fmwww.bc.edu/ec-p/data/wooldridge/gpa3.dta\")\n",
    "\n",
    "# Model with full interactions with female dummy (only for spring data)\n",
    "reg<-lm(cumgpa~female*(sat+hsperc+tothrs), data=gpa3, subset=(spring==1))\n",
    "summary(reg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the four terms involving the female dummy variable is very statistically significant; only the female&#8729;sat interaction has a t statistic close to two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The large standard errors on female and the interaction terms make it difficult to tell exactly how men and women differ. \n",
    "We must be very careful in interpreting the previous model because, \n",
    "in obtaining differences between women and men, the interaction terms must be taken into account. If we look only at the female variable, we would wrongly conclude that cumgpa is about .353 less for women than for men, holding other factors fixed. This is the estimated difference only when sat , hsperc , and tothrs are all set to zero, which is not close to being a possible scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At sat=1100, hsperc=10, and tothrs=50, the predicted difference between a woman and a man is -.353+(.00075*1100)-(.00055*10)-(.00011*50)=.461. That is, the female athlete is predicted to have a GPA that is almost one-half a point higher than the comparable male athlete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The so called Chow statistic allows us to formally test the null Hypothesis (that the interaction coefficients are zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Res.Df</th><th scope=col>RSS</th><th scope=col>Df</th><th scope=col>Sum of Sq</th><th scope=col>F</th><th scope=col>Pr(&gt;F)</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>362         </td><td>85.51507    </td><td>NA          </td><td>      NA    </td><td>      NA    </td><td>          NA</td></tr>\n",
       "\t<tr><td>358         </td><td>78.35451    </td><td> 4          </td><td>7.160561    </td><td>8.179112    </td><td>2.544637e-06</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       " Res.Df & RSS & Df & Sum of Sq & F & Pr(>F)\\\\\n",
       "\\hline\n",
       "\t 362          & 85.51507     & NA           &       NA     &       NA     &           NA\\\\\n",
       "\t 358          & 78.35451     &  4           & 7.160561     & 8.179112     & 2.544637e-06\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "Res.Df | RSS | Df | Sum of Sq | F | Pr(>F) | \n",
       "|---|---|\n",
       "| 362          | 85.51507     | NA           |       NA     |       NA     |           NA | \n",
       "| 358          | 78.35451     |  4           | 7.160561     | 8.179112     | 2.544637e-06 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  Res.Df RSS      Df Sum of Sq F        Pr(>F)      \n",
       "1 362    85.51507 NA       NA        NA           NA\n",
       "2 358    78.35451  4 7.160561  8.179112 2.544637e-06"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# F-Test from package \"car\". H0: the interaction coefficients are zero\n",
    "# matchCoefs(...) selects all coeffs with names containing \"female\"\n",
    "library(car)\n",
    "linearHypothesis(reg, matchCoefs(reg, \"female\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F-statistic is about 8.17, the p-value is zero to five decimal places which leads us to strongly reject H0. Thus men and women athletes do follow different GPA models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 7-5 A binary dependent variable: The linear probability model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, we have learned much about the properties and applicability of the multiple linear regression model. \n",
    "In the last several sections, we studied how, through the use of binary independent variables, \n",
    "we can incorporate qualitative information as explanatory variables in a multiple regression model. \n",
    "In all of the models up until now, the dependent variable y has had quantitative meaning \n",
    "(for example, y is a dollar amount, a test score, a percentage, or the logs of these). \n",
    "    What happens if we want to use multiple regression to explain a qualitative event?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simplest case, and one that often arises in practice, the event we would like to explain is a binary outcome. In other words, our dependent variable, y , takes on only two values: zero and one. For example, y can be defined to indicate whether an adult has a high school education; y can indicate whether a college student used illegal drugs during a given school year; or y can indicate whether a firm was taken over by another firm during a given year. In each of these examples, we can let y=1 denote one of the outcomes and y= 0 the other outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it mean to write down a multiple regression model, such as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "y=\\beta_0+\\beta_1*x_1+ \\ldots +\\beta_k*x_k\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "when y is a binary variable? Because y can take on only two values, $\\beta_j$ cannot be interpreted as the change in y given a one-unit increase in $x_j$ , holding all other factors fixed: y either changes from zero to one or from one to zero (or does not change). Nevertheless, the $\\beta_j$ still have useful interpretations. If we assume that the zero conditional mean assumption MLR.4 holds, that is, $E(u|x_1,\\ldots,x_k)=0$, then we have, as always,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "E(y|x)=\\beta_0+\\beta_1*x_1+ \\ldots +\\beta_k*x_k\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key point is that when y is a binary variable taking on the values zero and one, it is always true that $P(y=1|x)=\\beta_0+\\beta_1*x_1+\\ldots+\\beta_k*x_k$ : the probability of \"success\" that is, the probability that y=1 is the same as the expected value of y . Thus, we have the important equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(y=1|x)=\\beta_0+\\beta_1*x_1+ \\ldots +\\beta_k*x_k\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which says that the probability of success, say, $p(x)=P(y=1|x)$ , is a linear function of the $x_j$ . The previous equation is an example of a binary response model. Refer to chapter 17 for other examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiple linear regression model with a binary dependent variable is called the linear probability model (LPM) because the response probability is linear in the parameters $\\beta_j$, In the LPM, $\\beta_j$ measures the change in the probability of success when $x_j$ changes, holding other factors fixed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-7 Interpreting Regression Results with Discrete Dependent Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A binary response is the most extreme form of a discrete random variable: it takes on only two val- ues, zero and one. As we discussed in Section 7-5, the parameters in a linear probability model can be interpreted as measuring the change in the probability that y=1 due to a one-unit increase in an explanatory variable. We also discussed that, because y is a zero-one outcome, $P(y=1)=E(y)$ , and this equality continues to hold when we condition on explanatory variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interpret regression results generally, even in cases where y is discrete and takes on a small number of values, it is useful to remember the interpretation of OLS as estimating the effects of the $x_j$ on the expected (or average ) value of y . Generally, under Assumptions MLR.1 and MLR.4,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "E(y|x_1,x_2,\\ldots,x_k)=\\beta_0+\\beta_1*x_1+ \\ldots +\\beta_k*x_k\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, $\\beta_j$ is the effect of a ceteris paribus increase of $x_j$ on the expected value of y . As we discussed in Section 6-4, for a given set of $x_j$ values we interpret the predicted value, $\\hat\\beta_0+\\hat\\beta_1*x_1+\\ldots+\\hat\\beta_k*x_k$ , as an estimate of $E(y|x_1,x_2,\\ldots,x_k)$. Therefore, $\\hat\\beta_j$ is our estimate of how the average of y changes when $\\Delta x_j=1$ (keeping other factors fixed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incidentally, when y is discrete the linear model does not always provide the best estimates of partial effects on $E(y|x_1,x_2,\\ldots,x_k)$ Chapter 17 contains more advanced models and estimation methods that tend to fit the data better when the range of y is limited in some substantive way. Nevertheless, a linear model estimated by OLS often provides a good approximation to the true partial effects, at least on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "r"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
